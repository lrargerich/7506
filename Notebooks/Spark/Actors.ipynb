{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actors Network\n",
    "==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3.6\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python3.6\"\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"sample_app\")\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.254.36.12:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"foo\")\n",
    "# #Spark Config\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 3, 4, 6]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([0, 2, 3, 4, 6]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 166, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 55, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 169, in _read_with_length\n    return self.loads(obj)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 451, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 784, in _make_skel_func\n    closure = _reconstruct_closure(closures) if closures else None\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 776, in _reconstruct_closure\n    return tuple([_make_cell(v) for v in values])\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 166, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 55, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 169, in _read_with_length\n    return self.loads(obj)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 451, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 784, in _make_skel_func\n    closure = _reconstruct_closure(closures) if closures else None\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 776, in _reconstruct_closure\n    return tuple([_make_cell(v) for v in values])\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3f72e4875aea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m         \"\"\"\n\u001b[0;32m-> 1376\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    999\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 166, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 55, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 169, in _read_with_length\n    return self.loads(obj)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 451, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 784, in _make_skel_func\n    closure = _reconstruct_closure(closures) if closures else None\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 776, in _reconstruct_closure\n    return tuple([_make_cell(v) for v in values])\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 166, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 55, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 169, in _read_with_length\n    return self.loads(obj)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 451, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 784, in _make_skel_func\n    closure = _reconstruct_closure(closures) if closures else None\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 776, in _reconstruct_closure\n    return tuple([_make_cell(v) for v in values])\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "sc.parallelize([0, 2, 3, 4, 6]).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 15, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 166, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 55, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 169, in _read_with_length\n    return self.loads(obj)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 451, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 784, in _make_skel_func\n    closure = _reconstruct_closure(closures) if closures else None\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 776, in _reconstruct_closure\n    return tuple([_make_cell(v) for v in values])\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 166, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 55, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 169, in _read_with_length\n    return self.loads(obj)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 451, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 784, in _make_skel_func\n    closure = _reconstruct_closure(closures) if closures else None\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 776, in _reconstruct_closure\n    return tuple([_make_cell(v) for v in values])\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e12c63cf9e7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrdd2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m         \"\"\"\n\u001b[0;32m-> 1376\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    999\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 15, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 166, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 55, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 169, in _read_with_length\n    return self.loads(obj)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 451, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 784, in _make_skel_func\n    closure = _reconstruct_closure(closures) if closures else None\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 776, in _reconstruct_closure\n    return tuple([_make_cell(v) for v in values])\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 166, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 55, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 169, in _read_with_length\n    return self.loads(obj)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 451, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 784, in _make_skel_func\n    closure = _reconstruct_closure(closures) if closures else None\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 776, in _reconstruct_closure\n    return tuple([_make_cell(v) for v in values])\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "rdd2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "environ({'COLORFGBG': 'default;default', 'COLORTERM': 'rxvt-xpm', 'DBUS_SESSION_BUS_ADDRESS': 'unix:abstract=/tmp/dbus-Vtsivte8Uh,guid=b3a4629be3964d62d819468e5b801922', 'DEFAULTS_PATH': '/usr/share/gconf/i3.default.path', 'DERBY_HOME': '/usr/lib/jvm/java-8-oracle/db', 'DESKTOP_SESSION': 'i3', 'DESKTOP_STARTUP_ID': 'i3/urxvt/1374-4-pmitn010439_TIME21998256', 'DISPLAY': ':0', 'GDMSESSION': 'i3', 'GDM_LANG': 'en_US', 'GTK2_MODULES': 'overlay-scrollbar', 'GTK_MODULES': 'gail:atk-bridge', 'HOME': '/home/luis', 'J2REDIR': '/usr/lib/jvm/java-8-oracle/jre', 'J2SDKDIR': '/usr/lib/jvm/java-8-oracle', 'JAVA_HOME': '/usr/lib/jvm/java-8-oracle', 'LANG': 'en_US.UTF-8', 'LANGUAGE': 'en_US', 'LC_ADDRESS': 'es_AR.UTF-8', 'LC_IDENTIFICATION': 'es_AR.UTF-8', 'LC_MEASUREMENT': 'es_AR.UTF-8', 'LC_MONETARY': 'es_AR.UTF-8', 'LC_NAME': 'es_AR.UTF-8', 'LC_NUMERIC': 'es_AR.UTF-8', 'LC_PAPER': 'es_AR.UTF-8', 'LC_TELEPHONE': 'es_AR.UTF-8', 'LC_TIME': 'es_AR.UTF-8', 'LOGNAME': 'luis', 'MANDATORY_PATH': '/usr/share/gconf/i3.mandatory.path', 'NO_AT_BRIDGE': '1', 'OMF_CONFIG': '/home/luis/.config/omf', 'OMF_PATH': '/home/luis/.local/share/omf', 'PATH': '/usr/local/bin:/home/luis/.fzf/bin:/home/luis/bin:/home/luis/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/lib/jvm/java-8-oracle/bin:/usr/lib/jvm/java-8-oracle/db/bin:/usr/lib/jvm/java-8-oracle/jre/bin:/home/luis/anaconda3/bin:/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/bin', 'PWD': '/home/luis', 'PYTHONPATH': '/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python\\x1e/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip', 'QT_ACCESSIBILITY': '1', 'QT_LINUX_ACCESSIBILITY_ALWAYS_ON': '1', 'QT_QPA_PLATFORMTHEME': 'appmenu-qt5', 'SHELL': '/usr/bin/fish', 'SHLVL': '1', 'SPARK_HOME': '/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7', 'SSH_AGENT_PID': '1440', 'SSH_AUTH_SOCK': '/tmp/ssh-r8viC8aNU0bK/agent.1374', 'TERM': 'xterm-color', 'TRELLO_API_KEY': 'ed29fb85f4a36dd7efa9c9cf3e362301', 'TRELLO_API_SECRET': 'b037bfc4b982b3cc1e62f8cd363411253f554b32c90566f6479da4d3b1dbbe6a', 'TRELLO_EXPIRATION': 'never', 'TRELLO_NAME': 'TrelloWarrior', 'USER': 'luis', 'WINDOWID': '62914570', 'XAUTHORITY': '/home/luis/.Xauthority', 'XDG_CONFIG_DIRS': '/etc/xdg/xdg-i3:/etc/xdg', 'XDG_CURRENT_DESKTOP': 'i3', 'XDG_DATA_DIRS': '/usr/share/i3:/usr/local/share/:/usr/share/:/var/lib/snapd/desktop', 'XDG_GREETER_DATA_DIR': '/var/lib/lightdm-data/luis', 'XDG_RUNTIME_DIR': '/run/user/1001', 'XDG_SEAT': 'seat0', 'XDG_SEAT_PATH': '/org/freedesktop/DisplayManager/Seat0', 'XDG_SESSION_DESKTOP': 'i3', 'XDG_SESSION_ID': 'c2', 'XDG_SESSION_PATH': '/org/freedesktop/DisplayManager/Session0', 'XDG_SESSION_TYPE': 'x11', 'XDG_VTNR': '7', '__fish_bin_dir': '/usr/bin', '__fish_datadir': '/usr/share/fish', '__fish_help_dir': '/usr/share/doc/fish', '__fish_sysconfdir': '/etc/fish', 'JPY_PARENT_PID': '24983', 'CLICOLOR': '1', 'PAGER': 'cat', 'GIT_PAGER': 'cat', 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline', 'PYSPARK_PYTHON': 'python3.6', 'PYSPARK_DRIVER_PYTHON': 'python3.6'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 123, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 123, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a941e4db17e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrdd2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \"\"\"\n\u001b[0;32m-> 1056\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1045\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \"\"\"\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \"\"\"\n\u001b[1;32m    823\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 123, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 123, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "rdd2.count()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rdd1 = rdd1.map(lambda x: x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 0.0 failed 1 times, most recent failure: Lost task 2.0 in stage 0.0 (TID 2, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 166, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 55, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 169, in _read_with_length\n    return self.loads(obj)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 451, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 784, in _make_skel_func\n    closure = _reconstruct_closure(closures) if closures else None\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 776, in _reconstruct_closure\n    return tuple([_make_cell(v) for v in values])\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 166, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 55, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 169, in _read_with_length\n    return self.loads(obj)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 451, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 784, in _make_skel_func\n    closure = _reconstruct_closure(closures) if closures else None\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 776, in _reconstruct_closure\n    return tuple([_make_cell(v) for v in values])\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-351830dd7078>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrdd1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \"\"\"\n\u001b[1;32m    823\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 0.0 failed 1 times, most recent failure: Lost task 2.0 in stage 0.0 (TID 2, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 166, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 55, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 169, in _read_with_length\n    return self.loads(obj)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 451, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 784, in _make_skel_func\n    closure = _reconstruct_closure(closures) if closures else None\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 776, in _reconstruct_closure\n    return tuple([_make_cell(v) for v in values])\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 166, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 55, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 169, in _read_with_length\n    return self.loads(obj)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 451, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 784, in _make_skel_func\n    closure = _reconstruct_closure(closures) if closures else None\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 776, in _reconstruct_closure\n    return tuple([_make_cell(v) for v in values])\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 13.0 failed 1 times, most recent failure: Lost task 0.0 in stage 13.0 (TID 39, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 166, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 55, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 169, in _read_with_length\n    return self.loads(obj)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 451, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 784, in _make_skel_func\n    closure = _reconstruct_closure(closures) if closures else None\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 776, in _reconstruct_closure\n    return tuple([_make_cell(v) for v in values])\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 166, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 55, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 169, in _read_with_length\n    return self.loads(obj)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 451, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 784, in _make_skel_func\n    closure = _reconstruct_closure(closures) if closures else None\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 776, in _reconstruct_closure\n    return tuple([_make_cell(v) for v in values])\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-ebb12d27f1f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the Data (RAW Data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mactors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/imdb_actors_key_noheader.tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \"\"\"\n\u001b[0;32m-> 1056\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1045\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \"\"\"\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \"\"\"\n\u001b[1;32m    823\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 13.0 failed 1 times, most recent failure: Lost task 0.0 in stage 13.0 (TID 39, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 166, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 55, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 169, in _read_with_length\n    return self.loads(obj)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 451, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 784, in _make_skel_func\n    closure = _reconstruct_closure(closures) if closures else None\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 776, in _reconstruct_closure\n    return tuple([_make_cell(v) for v in values])\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 166, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 55, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 169, in _read_with_length\n    return self.loads(obj)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 451, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 784, in _make_skel_func\n    closure = _reconstruct_closure(closures) if closures else None\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 776, in _reconstruct_closure\n    return tuple([_make_cell(v) for v in values])\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# Load the Data (RAW Data)\n",
    "actors = sc.textFile('data/imdb_actors_key_noheader.tsv', 8)\n",
    "print(actors.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 14.0 failed 1 times, most recent failure: Lost task 0.0 in stage 14.0 (TID 44, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 166, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 55, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 169, in _read_with_length\n    return self.loads(obj)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 451, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 784, in _make_skel_func\n    closure = _reconstruct_closure(closures) if closures else None\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 776, in _reconstruct_closure\n    return tuple([_make_cell(v) for v in values])\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 166, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 55, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 169, in _read_with_length\n    return self.loads(obj)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 451, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 784, in _make_skel_func\n    closure = _reconstruct_closure(closures) if closures else None\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 776, in _reconstruct_closure\n    return tuple([_make_cell(v) for v in values])\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-f3b3e7005b00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mactors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    999\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 14.0 failed 1 times, most recent failure: Lost task 0.0 in stage 14.0 (TID 44, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 166, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 55, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 169, in _read_with_length\n    return self.loads(obj)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 451, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 784, in _make_skel_func\n    closure = _reconstruct_closure(closures) if closures else None\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 776, in _reconstruct_closure\n    return tuple([_make_cell(v) for v in values])\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 166, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 55, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 169, in _read_with_length\n    return self.loads(obj)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 451, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 784, in _make_skel_func\n    closure = _reconstruct_closure(closures) if closures else None\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 776, in _reconstruct_closure\n    return tuple([_make_cell(v) for v in values])\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "actors.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of 5 actors:\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 45, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 166, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 55, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 169, in _read_with_length\n    return self.loads(obj)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 451, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 784, in _make_skel_func\n    closure = _reconstruct_closure(closures) if closures else None\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 776, in _reconstruct_closure\n    return tuple([_make_cell(v) for v in values])\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1055)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 166, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 55, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 169, in _read_with_length\n    return self.loads(obj)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 451, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 784, in _make_skel_func\n    closure = _reconstruct_closure(closures) if closures else None\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 776, in _reconstruct_closure\n    return tuple([_make_cell(v) for v in values])\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1055)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-409a3b91da49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mactors_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"An example of 5 actors:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactors_key_cat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# Verificamos que el mundo siga en pie\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mkevin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactors_key_cat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3257\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    999\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luis/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 45, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 166, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 55, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 169, in _read_with_length\n    return self.loads(obj)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 451, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 784, in _make_skel_func\n    closure = _reconstruct_closure(closures) if closures else None\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 776, in _reconstruct_closure\n    return tuple([_make_cell(v) for v in values])\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1055)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 166, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 55, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 169, in _read_with_length\n    return self.loads(obj)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 451, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 784, in _make_skel_func\n    closure = _reconstruct_closure(closures) if closures else None\n  File \"/home/luis/Downloads/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 776, in _reconstruct_closure\n    return tuple([_make_cell(v) for v in values])\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1055)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# DISCLAIMER:\n",
    "# This code might (should) contain MANY bugs, you are expected to check everything twice.\n",
    "# Split by \\t\n",
    "actors_key_cat = actors.map(lambda x: tuple(x.split('\\t'))).map(lambda x:(int(x[0]),(x[1],x[3]))).cache()\n",
    "actors_key = actors.map(lambda x: tuple(x.split('\\t'))).map(lambda x:(int(x[0]),x[1])).cache()\n",
    "print(\"An example of 5 actors:\")\n",
    "print(actors_key_cat.take(5))\n",
    "# Verificamos que el mundo siga en pie\n",
    "kevin = actors_key_cat.filter(lambda x:x[0] == 3257)\n",
    "print(\"Our hero:\")\n",
    "print(kevin.collect())\n",
    "# So far so good\n",
    "# Now load the actors network\n",
    "network = sc.textFile('data/imdb_actor_edges.tsv')\n",
    "print(network.count())\n",
    "network = network.map(lambda x:tuple(x.split('\\t'))).map(lambda x:(int(x[0]),int(x[1]),int(x[2]))).cache()\n",
    "#network = network.map(lambda x:tuple(x.split('\\t'))).map(lambda x:(int(x[0]),int(x[1])))\n",
    "print(\"Example 5 records from network RDD:\")\n",
    "print(network.take(5))\n",
    "# Network has nodes in the form (actor1,actor2,times_acted_together)\n",
    "# actors_key_cat has nodes in the form (actorid,(name,category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Levesque, Paul Michael\":3062\n",
      "\"Calaway, Mark\":3029\n",
      "\"Ross, Jim (III)\":2824\n",
      "\"Lawler, Jerry\":2818\n",
      "\"McMahon, Vince\":2790\n",
      "\"Wight, Paul\":2731\n",
      "\"Jericho, Chris\":2722\n",
      "\"Benoit, Chris (I)\":2685\n",
      "\"Guerrero, Eddie\":2684\n",
      "\"Rock, The\":2675\n",
      "Kevin Bacon actua en: 232 peliculas\n"
     ]
    }
   ],
   "source": [
    "# Now we have the network in the right format, let's find the node with more different movies\n",
    "mn1 = network.flatMap(lambda x:[(x[0],x[2]),(x[1],x[2])])\n",
    "mn1 = mn1.reduceByKey(lambda x,y:x+y)\n",
    "mn1 = mn1.join(actors_key_cat)\n",
    "# Sorry about this\n",
    "mn1 = mn1.filter(lambda x:x[1][1][1]!='Adult')\n",
    "top_actors = mn1.takeOrdered(10,lambda x:-x[1][0])\n",
    "for actor in top_actors:\n",
    "    print(actor[1][1][0]+':'+str(actor[1][0]))\n",
    "bacon_movies = mn1.filter(lambda x:x[0]==3257).take(1)\n",
    "print(\"Kevin Bacon actua en: \"+str(bacon_movies[0][1][0])+ \" peliculas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actors at 1 degree of Kevin Bacon:101\n",
      "\"Danes, Claire\"\n",
      "\"Harris, Ed (I)\"\n",
      "\"Hoffman, Dustin\"\n",
      "\"Wilson, Luke (I)\"\n",
      "\"Pollak, Kevin\"\n",
      "\"Taylor, Elizabeth (I)\"\n",
      "\"Danson, Ted\"\n",
      "\"Hanks, Tom\"\n",
      "\"Grant, Hugh (I)\"\n",
      "\"Ice-T\"\n",
      "\"Bisset, Jacqueline\"\n",
      "\"Harden, Marcia Gay\"\n",
      "\"Sinise, Gary\"\n",
      "\"Bonham Carter, Helena\"\n",
      "\"Jackson, Janet (I)\"\n",
      "\"Quinlan, Kathleen\"\n",
      "\"Moore, Demi\"\n",
      "\"Shue, Elisabeth\"\n",
      "\"Gere, Richard\"\n",
      "\"Fishburne, Laurence\"\n",
      "\"Ford, Harrison (I)\"\n",
      "\"Jolie, Angelina\"\n",
      "\"Keaton, Michael\"\n",
      "\"Dern, Laura\"\n",
      "\"Leigh, Jennifer Jason\"\n",
      "\"Crudup, Billy\"\n",
      "\"Grunberg, Greg\"\n",
      "\"Penn, Sean\"\n",
      "\"Ryan, Meg\"\n",
      "\"Lauper, Cyndi\"\n",
      "\"Cher (I)\"\n",
      "\"Goldberg, Whoopi\"\n",
      "\"Robbins, Tim (I)\"\n",
      "\"Roberts, Julia\"\n",
      "\"Howard, Clint\"\n",
      "\"Dunn, Kevin (I)\"\n",
      "\"Paxton, Bill\"\n",
      "\"Lange, Jessica\"\n",
      "\"Fonda, Bridget\"\n",
      "\"Smith Jr., Eddie Bo\"\n",
      "\"Spears, Britney\"\n",
      "\"Wahlberg, Mark (I)\"\n",
      "\"Parker, Sarah Jessica\"\n",
      "\"Matlin, Marlee\"\n",
      "\"Allen, Karen (I)\"\n",
      "\"Brolin, Josh\"\n",
      "\"Dillon, Matt (I)\"\n",
      "\"Crawford, Thomas (I)\"\n",
      "\"Helgeland, Brian\"\n",
      "\"Gibson, Mel (I)\"\n",
      "\"Spacey, Kevin\"\n",
      "\"Lane, Diane (I)\"\n",
      "\"Reitman, Ivan\"\n",
      "\"Norwood, Brandy\"\n",
      "\"Travolta, John\"\n",
      "\"Riegert, Peter\"\n",
      "\"Ramis, Harold\"\n",
      "\"MacLaine, Shirley\"\n",
      "\"McRobbie, Peter\"\n",
      "\"Hopper, Dennis\"\n",
      "\"Eastwood, Clint\"\n",
      "\"Costner, Kevin\"\n",
      "\"Furst, Stephen\"\n",
      "\"De Niro, Robert\"\n",
      "\"McGill, Bruce\"\n",
      "\"Tomlin, Lily\"\n",
      "\"Matheson, Tim (I)\"\n",
      "\"Madonna (I)\"\n",
      "\"Driver, Minnie\"\n",
      "\"Silverstone, Alicia\"\n",
      "\"Sedgwick, Kyra\"\n",
      "\"Hoffman, Philip Seymour\"\n",
      "\"Hounsou, Djimon\"\n",
      "\"Crawford, Cindy (I)\"\n",
      "\"Verhoeven, Paul (I)\"\n",
      "\"Rimes, LeAnn\"\n",
      "\"Newton-John, Olivia\"\n",
      "\"Slotnick, Joey\"\n",
      "\"Jameson, Jenna\"\n",
      "\"Keitel, Harvey\"\n",
      "\"Murphy, Eddie (I)\"\n",
      "\"Linney, Laura\"\n",
      "\"Vella, Vinny\"\n",
      "\"Douglas, Michael (I)\"\n",
      "\"Bon Jovi, Jon\"\n",
      "\"Landis, John (I)\"\n",
      "\"Woodruff Jr., Tom\"\n",
      "\"Masterson, Mary Stuart\"\n",
      "\"Willis, Bruce (I)\"\n",
      "\"Schiffer, Claudia\"\n",
      "\"Fonda, Peter (I)\"\n",
      "\"Cruise, Tom\"\n",
      "\"Bratt, Benjamin (I)\"\n",
      "\"Carrey, Jim\"\n",
      "\"Schwarzenegger, Arnold\"\n",
      "\"Scott, Jill (II)\"\n",
      "\"Renfro, Brad\"\n",
      "\"Douglas, Illeana\"\n",
      "\"Vernon, John (I)\"\n",
      "\"Muniz, Frankie\"\n",
      "\"Dickens, Kim\"\n"
     ]
    }
   ],
   "source": [
    "# One degrees of kevin bacon\n",
    "# We want a to be with 3257\n",
    "kevin1 = network.filter(lambda x:x[0]==3257 or x[1]==3257).map(lambda x:x[0] if x[1]==3257 else x[1])\n",
    "kevin1_names = kevin1.map(lambda x:(x,1)).join(actors_key).map(lambda x:x[1][1])\n",
    "# Number of actors with Bacon Degree 1\n",
    "print(\"Actors at 1 degree of Kevin Bacon:\"+str(kevin1_names.count()))\n",
    "bacon1_actors = kevin1_names.collect()\n",
    "for actor in bacon1_actors:\n",
    "    print(actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(17776, (999, '-', [17777, 17778], 0))]\n",
      "Total number of nodes in our graph:17577\n",
      "Number of nodes with Bacon Number:101/17577\n",
      "Number of nodes with Bacon Number:2491/17577\n",
      "Number of nodes with Bacon Number:0 9650/17577\n",
      "Number of nodes with Bacon Number:1 14463/17577\n",
      "Number of nodes with Bacon Number:2 16459/17577\n",
      "Number of nodes with Bacon Number:3 16951/17577\n",
      "Number of nodes with Bacon Number:4 17270/17577\n",
      "Number of nodes with Bacon Number:5 17445/17577\n",
      "Number of nodes with Bacon Number:6 17455/17577\n",
      "Number of nodes with Bacon Number:7 17455/17577\n",
      "[(5222, (5, 'DONE', [8000, 4672, 4676, 5224, 9611, 8016, 4787, 8020, 4692, 16276, 4789, 4767], 9611))]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "# N degrees of Kevin Bacon\n",
    "# We want to build an adjacency list for the actors graph\n",
    "# so if we have (A,B)(A,C)(B,C) we want (A,[B,C]),(B,[C]), etc\n",
    "\n",
    "# First we convert each (A,B) into (A,B) and (B,A)\n",
    "adjlist = network.flatMap(lambda x:[(x[0],x[1]),(x[1],x[0])])\n",
    "\n",
    "#foo = adjlist.groupByKey().filter(lambda x:x[0]==3257).map(lambda x:(x[0],(set(x[1]))))\n",
    "#print foo.take(1)\n",
    "# Now we want to groupByKey and remove duplicate values from the list of neighbors\n",
    "adjlist = adjlist.groupByKey().map(lambda x:(x[0],(999,'-',list(set(x[1])),0))).cache()\n",
    "graph = adjlist.map(lambda x:(x[0],(x[1][2]))).cache()\n",
    "\n",
    "print( adjlist.take(1) )\n",
    "\n",
    "total_nodes = adjlist.count()\n",
    "\n",
    "print(\"Total number of nodes in our graph:\"+str(total_nodes))\n",
    "\n",
    "# Now our RDD is in the form [(node,(distance_from_kevin,status,[neighbors])),....(node,(distance_from_kevin,status,[neighbors]))]\n",
    "# The next step is to mark all nodes that are neighbors of Kevin Bacon as being at distance 1 and to be processed\n",
    "# to do this we take kevin's node and generate new nodes for the neighbors in the form (node_id,(1,'P',[]))\n",
    "# then the reduceByKey phase will just join all the nodes into a single one (hopefully)\n",
    "# we define proc_node as the function that processes a node and we'll start from 3257 of course (Kevin)\n",
    "\n",
    "def proc_node(node):\n",
    "    # We receive a node in the form (nodeid,distance_from_kevin,status,list_of_neighbors,trace)\n",
    "    # We generate new nodes for each neighbor adding 1 to the distance of the current node (!)\n",
    "    if node[1][0]==999:\n",
    "        new_distance = 1\n",
    "    else:\n",
    "        new_distance = node[1][0]+1\n",
    "    ret = []\n",
    "    for neighbor in node[1][2]:\n",
    "        ret.append((neighbor,(new_distance,'P',[],node[0])))\n",
    "    # We also update the node as already processed!\n",
    "    ret.append((node[0],(node[1][0],'DONE',node[1][2],node[1][3])))\n",
    "    return ret\n",
    "\n",
    "# Now we define the reduce function\n",
    "def reduce_nodes(n1,n2):\n",
    "    # We receive two nodes in the form (distance_from_kevin,status,list_of_neighbors,trace)\n",
    "    # We keep the minimum distance\n",
    "    # if one of the nodes is marked as 'DONE' we keep 'DONE'\n",
    "    # if one of the nodes is marked as 'P' we keep 'P'\n",
    "    # we keep the longest list of neighbors (some can be empty)\n",
    "    # We keep the trace of the minimum distance\n",
    "    new_status = ''\n",
    "    if n1[1]=='P' or n2[1]=='P':\n",
    "        new_status = 'P'\n",
    "    if n1[1]=='DONE' or n2[1]=='DONE':\n",
    "        new_status = 'DONE'\n",
    "    if n1[0]<n2[0]:\n",
    "        new_distance = n1[0]\n",
    "        trace = n1[3]\n",
    "    else:\n",
    "        new_distance = n2[0]\n",
    "        trace = n2[3]\n",
    "    new_list = n1[2]\n",
    "    if len(n2[2])>len(n1[2]):\n",
    "        new_list = n2[2]\n",
    "    return (new_distance,new_status,new_list,trace)\n",
    "\n",
    "# Our first task is to process only Kevin's node\n",
    "adjlist = adjlist.flatMap(lambda x:proc_node(x) if x[0]==3257 else [x])\n",
    "adjlist = adjlist.reduceByKey(reduce_nodes)\n",
    "k_with_number = adjlist.filter(lambda x:x[1][0]<999)\n",
    "print(\"Number of nodes with Bacon Number:\"+str(k_with_number.count())+\"/\"+str(total_nodes))\n",
    "\n",
    "# Our second step is to process all the 'P' nodes\n",
    "adjlist = adjlist.flatMap(lambda x:proc_node(x) if x[1][1]=='P' else [x])\n",
    "adjlist = adjlist.reduceByKey(reduce_nodes)\n",
    "k_with_number = adjlist.filter(lambda x:x[1][0]<999)\n",
    "print(\"Number of nodes with Bacon Number:\"+str(k_with_number.count())+\"/\"+str(total_nodes))\n",
    "\n",
    "for i in range(0,8):\n",
    "    adjlist = adjlist.flatMap(lambda x:proc_node(x) if x[1][1]=='P' else [x])\n",
    "    adjlist = adjlist.reduceByKey(reduce_nodes)\n",
    "    k_with_number = adjlist.filter(lambda x:x[1][0]<999)\n",
    "    print(\"Number of nodes with Bacon Number:\"+str(i)+\" \"+str(k_with_number.count())+\"/\"+str(total_nodes))\n",
    "\n",
    "print(adjlist.filter(lambda x:x[0]==5222).collect())\n",
    "# El camino es 5222-9611-4763-8473-8468\n",
    "# Andrea Pietra - Pauls Gaston - Henriksen Lance - WoodRuff Jr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actors:17577\n",
      "Average degree:16.33236616032315\n"
     ]
    }
   ],
   "source": [
    "# The average degree of the network is?\n",
    "total_deg = network.flatMap(lambda x:[(x[0],1),(x[1],1)]).reduceByKey(lambda x,y:x+y)\n",
    "num_actors = total_deg.count()\n",
    "print(\"Number of actors:\"+str(num_actors))\n",
    "acum_deg = total_deg.map(lambda x:x[1]).reduce(lambda x,y:x+y)\n",
    "print(\"Average degree:\"+str((float(acum_deg)/num_actors)/2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe770d76fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7cAAAEICAYAAACEZwOKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe770d76438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distribution of degrees\n",
    "# We start with an RDD total_deg that has the form (actor_id, degree) and we want to turn it into something like\n",
    "# (degree, times)\n",
    "dist_deg = total_deg.map(lambda x:(x[1],1)).reduceByKey(lambda x,y:x+y).sortByKey(lambda x:-x[1])\n",
    "#print dist_deg.take(10)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "\n",
    "degree_freq = dist_deg.collect()\n",
    "degrees = [i for i,j in degree_freq]\n",
    "freqs = [j for i,j in degree_freq]\n",
    "\n",
    "#print degrees\n",
    "#print freqs\n",
    "\n",
    "plt.close()\n",
    "plt.clf()\n",
    "plt.figure(figsize=(16,4))\n",
    "y_pos = np.arange(len(degrees))\n",
    "#print y_pos\n",
    "plt.bar(y_pos, freqs, alpha=0.5)\n",
    "#plt.xticks(y_pos, my_words)\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Degree')\n",
    "plt.show() \n",
    "#plt.savefig('degree_dist.png')\n",
    "#Image(\"degree_dist.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1557, 1664, 1808), (1577, 1644, 6254), (70, 1628, 1685), (1590, 1950, 7513), (1876, 1950, 7513)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[124] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PART I\n",
    "# Now let's find triangles in the actors network!\n",
    "# We want to find pairs (A,B) where A never worked together with B but\n",
    "# they have worked together with a common actor C many times.\n",
    "# in other words (A,B) would close many triangles.\n",
    "\n",
    "# First step put in format (A,B) where A<B for all tuples\n",
    "tri = network.map(lambda x:tuple(sorted([x[0],x[1]]))).cache()\n",
    "# Second step for each (A,B) create (A,(B,1)) and (B,(A,0))\n",
    "tri = tri.flatMap(lambda x:[(x[1],(x[0],1)),(x[0],(x[1],0))]).cache()\n",
    "\n",
    "def proc(key,vals):\n",
    "    l0 = [node[0] for node in vals if node[1]==1]\n",
    "    l1 = [node[0] for node in vals if node[1]==0]\n",
    "    join = list(itertools.product(l0,l1))\n",
    "    res = [(node[0],key,node[1]) for node in join]\n",
    "    return res\n",
    "#tri = tri.groupByKey().map(lambda x:(x[0],list(x[1])))\n",
    "\n",
    "# THIS CAN TAKE A WHILE!\n",
    "tri = tri.groupByKey().flatMap(lambda x:proc(x[0],x[1])).distinct()\n",
    "print(tri.take(5))\n",
    "#print tri.groupByKey().map(lambda x:(x[0],list(x[1]))).take(5)\n",
    "#proc(99,[(16621, 0), (16697, 0), (10, 0), (23, 0), (12181, 0), (15304, 0), (16693, 0), (0, 1), (16418, 0), (31, 0), (15297, 0), (9, 0), (43, 0), (16699, 0), (27, 0), (16695, 0), (4, 1), (5, 1), (33, 0), (18, 0), (1, 1), (14, 0), (15301, 0), (25, 0), (56, 0)])\n",
    "tri.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((106, 1821), 1), ((1614, 2271), 1), ((1577, 1980), 1), ((1596, 1879), 1), ((177, 1792), 1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[130] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PART II \n",
    "# Continue, now we have an RDD where each record is in the form (A,B,C) meaning A worked with B and B worked with C \n",
    "# but we don't know if A worked with C\n",
    "# so we are going to generate something in the form ((A,C),1) meaning that if (A,C) exists then it closes 1 triangle.\n",
    "tri2 = tri.map(lambda x:((x[0],x[2]),1))\n",
    "print(tri2.take(5))\n",
    "# Now we just do a reduce by key\n",
    "most_triangles = tri2.reduceByKey(lambda x,y:x+y)\n",
    "most_triangles.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Diaz, Cameron\" and \"Myers, Mike (I)\" would close 136 triangles.\n",
      "\"Barrymore, Drew\" and \"Myers, Mike (I)\" would close 130 triangles.\n",
      "\"Hanks, Tom\" and \"Stone, Sharon (I)\" would close 127 triangles.\n",
      "\"Ferrell, Will\" and \"Myers, Mike (I)\" would close 126 triangles.\n",
      "\"Kuwashima, Houko\" and \"Seki, Tomokazu\" would close 110 triangles.\n",
      "\"Carrey, Jim\" and \"Stone, Sharon (I)\" would close 109 triangles.\n",
      "\"Tma, Yumi\" and \"Seki, Tomokazu\" would close 107 triangles.\n",
      "\"Hayashibara, Megumi\" and \"Seki, Tomokazu\" would close 107 triangles.\n",
      "\"Stiller, Ben\" and \"Myers, Mike (I)\" would close 106 triangles.\n",
      "\"Crystal, Billy\" and \"Myers, Mike (I)\" would close 105 triangles.\n",
      "\"Diaz, Cameron\" and \"Berry, Halle\" would close 105 triangles.\n",
      "\"Affleck, Ben\" and \"Stone, Sharon (I)\" would close 100 triangles.\n",
      "\"Cruise, Tom\" and \"Stone, Sharon (I)\" would close 98 triangles.\n",
      "\"Fallon, Jimmy\" and \"Myers, Mike (I)\" would close 97 triangles.\n",
      "\"Carrey, Jim\" and \"Myers, Mike (I)\" would close 96 triangles.\n",
      "\"Diaz, Cameron\" and \"Spielberg, Steven\" would close 96 triangles.\n",
      "\"Hanks, Tom\" and \"Myers, Mike (I)\" would close 93 triangles.\n",
      "\"Crystal, Billy\" and \"Stone, Sharon (I)\" would close 93 triangles.\n",
      "\"Freeman, Crispin\" and \"McGlynn, Mary Elizabeth\" would close 90 triangles.\n",
      "\"Freeman, Crispin\" and \"Maddalena, Julie\" would close 88 triangles.\n",
      "\"Affleck, Ben\" and \"Myers, Mike (I)\" would close 88 triangles.\n",
      "\"Streisand, Barbra\" and \"Stone, Sharon (I)\" would close 88 triangles.\n",
      "\"Kuwashima, Houko\" and \"Yamaguchi, Kappei\" would close 87 triangles.\n",
      "\"Shannon, Molly\" and \"Myers, Mike (I)\" would close 87 triangles.\n",
      "\"Hayashibara, Megumi\" and \"Yamaguchi, Kappei\" would close 86 triangles.\n",
      "\"Hanks, Tom\" and \"Spielberg, Steven\" would close 86 triangles.\n",
      "\"Freeman, Crispin\" and \"Doyle, Peter (IV)\" would close 85 triangles.\n",
      "\"Freeman, Crispin\" and \"Seki, Tomokazu\" would close 85 triangles.\n",
      "\"Baldwin, Alec\" and \"Myers, Mike (I)\" would close 85 triangles.\n",
      "\"Williams, Robin (I)\" and \"Myers, Mike (I)\" would close 85 triangles.\n",
      "\"Williams, Robin (I)\" and \"Spielberg, Steven\" would close 85 triangles.\n",
      "\"Carrey, Jim\" and \"Hawn, Goldie\" would close 84 triangles.\n",
      "\"Ueda, Yji\" and \"Seki, Tomokazu\" would close 84 triangles.\n",
      "\"Mitsuishi, Kotono\" and \"Seki, Tomokazu\" would close 84 triangles.\n",
      "\"Carrey, Jim\" and \"Berry, Halle\" would close 84 triangles.\n",
      "\"Baldwin, Alec\" and \"Stone, Sharon (I)\" would close 84 triangles.\n",
      "\"Blum, Steven (I)\" and \"James, Milton\" would close 83 triangles.\n",
      "\"Lee, Wendee\" and \"James, Milton\" would close 83 triangles.\n",
      "\"Stiller, Ben\" and \"Spielberg, Steven\" would close 83 triangles.\n",
      "\"Affleck, Ben\" and \"Berry, Halle\" would close 83 triangles.\n",
      "\"Lee, Wendee\" and \"Miller, Matt K.\" would close 83 triangles.\n",
      "\"De Niro, Robert\" and \"Stone, Sharon (I)\" would close 83 triangles.\n",
      "\"Freeman, Crispin\" and \"Epcar, Richard\" would close 82 triangles.\n",
      "\"Papenbrook, Bob\" and \"James, Milton\" would close 82 triangles.\n",
      "\"Schwarzenegger, Arnold\" and \"Myers, Mike (I)\" would close 82 triangles.\n",
      "\"Blum, Steven (I)\" and \"McGlynn, Mary Elizabeth\" would close 82 triangles.\n",
      "\"Freeman, Crispin\" and \"Spevack, Melodee\" would close 82 triangles.\n",
      "\"Tma, Yumi\" and \"Yamaguchi, Kappei\" would close 81 triangles.\n",
      "\"Ferrell, Will\" and \"Short, Martin (I)\" would close 81 triangles.\n",
      "\"Shannon, Molly\" and \"Short, Martin (I)\" would close 81 triangles.\n"
     ]
    }
   ],
   "source": [
    "# PART III\n",
    "# print most_triangles.takeOrdered(5,lambda x:-x[1])\n",
    "# Now we want to check if this triangles exist or not so we go to network and create something like ((actor1,actor2),1)\n",
    "n2 = network.map(lambda x:((x[0],x[1]),'Y'))\n",
    "# We append this to the most_triangles RDD both have the same format\n",
    "n2 = n2.union(most_triangles)\n",
    "# Now we do a groupByKey \n",
    "n2 = n2.groupByKey()\n",
    "n2 = n2.map(lambda x:(x[0],list(x[1]))).filter(lambda x:len(x[1])==1 and x[1][0]!='Y').map(lambda x:((x[0][0],x[0][1]),x[1][0]))\n",
    "#print n2.takeOrdered(10,lambda x:-x[1])\n",
    "# Now we have to do the joins and remove some actors\n",
    "n3 = n2.map(lambda x:(x[0][0],(x[0][1],x[1])))\n",
    "n3 = n3.join(actors_key_cat).filter(lambda x:x[1][1][1]!='Adult' and x[1][1][1]!='Action')\n",
    "n3 = n3.map(lambda x:(x[1][0][0],(x[1][0][1],x[1][1])))\n",
    "n3 = n3.join(actors_key_cat)\n",
    "n3 = n3.map(lambda x:(x[1][0][1],x[1][1],x[1][0][0]))\n",
    "pairs = n3.takeOrdered(50,lambda x:-x[2])\n",
    "for pair in pairs:\n",
    "    print(pair[0][0]+' and '+pair[1][0]+' would close '+str(pair[2])+' triangles.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We took a sample of:5222 nodes\n",
      "Round: 0 Total number of nodes:10444\n",
      "Round: 1 Total number of nodes:15666\n",
      "Round: 2 Total number of nodes:20888\n",
      "Round: 3 Total number of nodes:26110\n",
      "Round: 4 Total number of nodes:31332\n",
      "Round: 5 Total number of nodes:36554\n",
      "Round: 6 Total number of nodes:41776\n",
      "Round: 7 Total number of nodes:46998\n",
      "Round: 8 Total number of nodes:52220\n",
      "Round: 9 Total number of nodes:57442\n",
      "Round: 10 Total number of nodes:62664\n",
      "Round: 11 Total number of nodes:67886\n",
      "Round: 12 Total number of nodes:73108\n",
      "Round: 13 Total number of nodes:78330\n",
      "Round: 14 Total number of nodes:83552\n",
      "Round: 15 Total number of nodes:88774\n",
      "Round: 16 Total number of nodes:93996\n",
      "Round: 17 Total number of nodes:99218\n",
      "Round: 18 Total number of nodes:104440\n",
      "Round: 19 Total number of nodes:109662\n",
      "Round: 20 Total number of nodes:114884\n",
      "Round: 21 Total number of nodes:120106\n",
      "Round: 22 Total number of nodes:125328\n",
      "Round: 23 Total number of nodes:130550\n",
      "Round: 24 Total number of nodes:135772\n",
      "Round: 25 Total number of nodes:140994\n",
      "Round: 26 Total number of nodes:146216\n",
      "Round: 27 Total number of nodes:151438\n",
      "Round: 28 Total number of nodes:156660\n",
      "Round: 29 Total number of nodes:161882\n",
      "Round: 30 Total number of nodes:167104\n",
      "Round: 31 Total number of nodes:172326\n",
      "Round: 32 Total number of nodes:177548\n",
      "Round: 33 Total number of nodes:182770\n",
      "Round: 34 Total number of nodes:187992\n",
      "Round: 35 Total number of nodes:193214\n",
      "Round: 36 Total number of nodes:198436\n",
      "Round: 37 Total number of nodes:203658\n",
      "Round: 38 Total number of nodes:208880\n",
      "Round: 39 Total number of nodes:214102\n",
      "Round: 40 Total number of nodes:219324\n",
      "Round: 41 Total number of nodes:224546\n",
      "Round: 42 Total number of nodes:229768\n",
      "Round: 43 Total number of nodes:234990\n",
      "Round: 44 Total number of nodes:240212\n",
      "Round: 45 Total number of nodes:245434\n",
      "Round: 46 Total number of nodes:250656\n",
      "Round: 47 Total number of nodes:255878\n",
      "Round: 48 Total number of nodes:261100\n",
      "Round: 49 Total number of nodes:266322\n",
      "----------------------------------------------------------------------\n",
      "1. \"Lopez, Jennifer (I)\" (Music) = 213\n",
      "2. \"Hanks, Tom\" (Family) = 205\n",
      "3. \"Jackson, Samuel L.\" (Drama) = 203\n",
      "4. \"Berry, Halle\" (Family) = 193\n",
      "5. \"Goldberg, Whoopi\" (Comedy) = 192\n",
      "6. \"Stiller, Ben\" (Comedy) = 192\n",
      "7. \"Myers, Mike (I)\" (Comedy) = 188\n",
      "8. \"Davis, Mark (V)\" (Adult) = 184\n",
      "9. \"Diaz, Cameron\" (Drama) = 172\n",
      "10. \"Baldwin, Alec\" (Comedy) = 172\n",
      "11. \"Travolta, John\" (Drama) = 168\n",
      "12. \"Schwarzenegger, Arnold\" (Family) = 163\n",
      "13. \"Barrymore, Drew\" (Drama) = 160\n",
      "14. \"Stone, Sharon (I)\" (Family) = 160\n",
      "15. \"Sanders, Alex (I)\" (Adult) = 158\n",
      "16. \"Williams, Robin (I)\" (Comedy) = 157\n",
      "17. \"Cruise, Tom\" (Music) = 154\n",
      "18. \"Smith, Will (I)\" (Music) = 153\n",
      "19. \"Berland, Franois\" (Drama) = 152\n",
      "20. \"North, Peter (I)\" (Adult) = 151\n",
      "21. \"Jeremy, Ron\" (Adult) = 151\n",
      "22. \"Boy, T.T.\" (Adult) = 150\n",
      "23. \"Tedeschi, Tony\" (Adult) = 146\n",
      "24. \"Carrey, Jim\" (Family) = 146\n",
      "25. \"Kidman, Nicole\" (Family) = 146\n",
      "26. \"Ford, Harrison (I)\" (Thriller) = 144\n",
      "27. \"Spears, Britney\" (Music) = 144\n",
      "28. \"Marcus, Mr.\" (Adult) = 144\n",
      "29. \"Sarandon, Susan\" (Drama) = 142\n",
      "30. \"Rock, Chris (I)\" (Comedy) = 141\n",
      "31. \"Voyeur, Vince\" (Adult) = 140\n",
      "32. \"Richardson, Kevin Michael\" (Sci-Fi) = 138\n",
      "33. \"Hoffman, Dustin\" (Thriller) = 137\n",
      "34. \"Affleck, Ben\" (Comedy) = 135\n",
      "35. \"Spielberg, Steven\" (Family) = 135\n",
      "36. \"Spacey, Kevin\" (Drama) = 133\n",
      "37. \"Crystal, Billy\" (Comedy) = 133\n",
      "38. \"Lawrence, Joel (II)\" (Adult) = 132\n",
      "39. \"Depardieu, Grard\" (Comedy) = 132\n",
      "40. \"Cage, Nicolas\" (Thriller) = 131\n",
      "41. \"Ferrell, Will\" (Comedy) = 129\n",
      "42. \"De Niro, Robert\" (Comedy) = 129\n",
      "43. \"Douglas, Michael (I)\" (Family) = 128\n",
      "44. \"Kelly, Jill\" (Adult) = 128\n",
      "45. \"McCartney, Paul\" (Music) = 127\n",
      "46. \"Dough, Jon\" (Adult) = 126\n",
      "47. \"Sting\" (Music) = 126\n",
      "48. \"Blum, Steven (I)\" (Sci-Fi) = 125\n",
      "49. \"Cannon, Chris (III)\" (Adult) = 124\n",
      "50. \"Bune, Tyce\" (Adult) = 124\n"
     ]
    }
   ],
   "source": [
    "# Centrality by Random Walks!\n",
    "# Select some random nodes from the graph\n",
    "# From those nodes start a random walk\n",
    "# Then just compute 1 for each node we have found!\n",
    "import numpy as np\n",
    "\n",
    "# First step take a random Sample from adjlist with replacement\n",
    "my_sample = graph.sample(True,0.3)\n",
    "\n",
    "# The nodes we took are part of the random walks\n",
    "all_the_nodes = my_sample.map(lambda x:(x[0],1))\n",
    "\n",
    "print(\"We took a sample of:\"+str(my_sample.count())+\" nodes\")\n",
    "\n",
    "# This function gets a node and returns a new node selecting a random neighbor\n",
    "# From each node in the sample select a random neighbor\n",
    "def pick_random_neighbor(node):\n",
    "    neighbor = np.random.choice(node[1])\n",
    "    return (neighbor,'node')\n",
    "\n",
    "for i in range(0,50):\n",
    "    # Pick a random neighbor from each node\n",
    "    my_sample = my_sample.map(lambda x:pick_random_neighbor(x)).cache()\n",
    "    # Join with graph to recover the list of neighbors\n",
    "    my_sample = my_sample.join(graph).map(lambda x:(x[0],x[1][1])).cache()\n",
    "    # Add the new nodes to the list of visited nodes\n",
    "    just_nodes = my_sample.map(lambda x:(x[0],1))\n",
    "    all_the_nodes = all_the_nodes.union(just_nodes).cache()\n",
    "    print(\"Round: \"+str(i)+\" Total number of nodes:\"+str(all_the_nodes.count()))\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "all_the_nodes = all_the_nodes.reduceByKey(lambda x,y:x+y)\n",
    "nodes_with_names = all_the_nodes.join(actors_key_cat)\n",
    "#print nodes_with_names.take(5)\n",
    "central_actors =  nodes_with_names.takeOrdered(50,lambda x:-x[1][0])\n",
    "i=0\n",
    "for actor in central_actors:\n",
    "    i=i+1\n",
    "    print(str(i)+\". \"+actor[1][1][0]+\" (\"+actor[1][1][1]+\") = \"+str(actor[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
